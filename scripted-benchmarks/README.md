# scripted-benchmarks

In this directory, we present the data used for Section III in the paper, which examines a more rigid process when exploring the potential applications for LLMs in hardware design. Here, each model chats are separated by subdirectory.

Note that because Google's Bard and HuggingFace's HuggingChat produced unsatisfactory results, we did not perform the full set of benchmarks for these models, instead only providing the results for the first benchmark experiment (the shift register). 

gpt-3p5 (ChatGPT-3.5) and gpt-4 (ChatGPT-4) instead produced satisfactory results, and so we performed the full set of benchmarks for these models. These are divided by test into subdirectory.